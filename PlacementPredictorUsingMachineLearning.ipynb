{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "  \n",
    "import matplotlib.pyplot as plt  \n",
    "import plotly.express as px  \n",
    "from plotly.offline import init_notebook_mode, iplot  \n",
    "init_notebook_mode(connected=True)  \n",
    "  \n",
    "from sklearn.decomposition import PCA  \n",
    "  \n",
    "from sklearn. preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.model_selection import cross_val_score  \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  \n",
    "  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  \n",
    "  \n",
    "import pickle  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv('college_place.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting yo know the shape of the dataset (rows,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing 4 rows of the dataset at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the data typs of the columns that are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting to know the detailed information of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Descriptions of the numerical values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessign phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Checing for the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removal of the duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate rows check\n",
    "print(dataframe.duplicated().sum())\n",
    "\n",
    "#removal of duplicate rows\n",
    "dataframe.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking if the duplicate rows are removed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "### Exploratory data analysis is an important stage in machine learning, which involves examining and visualizing the data to learn more about its composition, traits, and trends. It is carried out prior to developing the actual machine learning model and is crucial for spotting possible difficulties and choosing the right preprocessing and feature engineering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(dataframe, x=\"CGPA\", y=\"Internships\", color=\"PlacedOrNot\",  \n",
    "                 hover_data=['CGPA'])  \n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Histogram for the count of place and not placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataframe, x='PlacedOrNot', color='PlacedOrNot', barmode='group')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Chart: Percentage pie chart of placed or not Placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.pie(dataframe, values=dataframe['PlacedOrNot'].value_counts().values, names=dataframe['PlacedOrNot'].value_counts().index, title='Placed Vs Not Placed')  \n",
    "figure.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the age of the youngest and Eldest student who is placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Age of Placed Person: \",dataframe[(dataframe['Age'] == dataframe['Age'].max()) & (dataframe['PlacedOrNot']==1)]['Age'].values[0])  \n",
    "print(\"Min Age of Placed Person: \",dataframe[(dataframe['Age'] == dataframe['Age'].min()) & (dataframe['PlacedOrNot']==1)]['Age'].values[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Maximum and the Minimum number of internships done by the student who is placed.   \n",
    "### We will also print the Maximum and Minimum number of students who did the max internship and the minimum number of internships.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Internships Done by the Placed Student: \",dataframe[(dataframe['Internships'] == dataframe['Internships'].max()) & (dataframe['PlacedOrNot']==1)]['Internships'].values[0])  \n",
    "print(\"No of students who did max Internships and are placed: \",dataframe[(dataframe['Internships'] == dataframe['Internships'].max()) & (dataframe['PlacedOrNot']==1)]['Internships'].value_counts().values[0])  \n",
    "  \n",
    "print(\"Min Internships Done by the Placed Person: \",dataframe[(dataframe['Internships'] == dataframe['Internships'].min()) & (dataframe['PlacedOrNot']==1)]['Internships'].values[0])  \n",
    "print(\"No of students who did min Internships and are placed: \",dataframe[(dataframe['Internships'] == dataframe['Internships'].min()) & (dataframe['PlacedOrNot']==1)]['Internships'].value_counts().values[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Maximum and Minimum number of CGPA obtained by the student who is placed.  \n",
    "### We will also print the Maximum and the Minimum number of students who got the max CGPA and minimum CGPA who are placed.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max CGPA of Placed Student: \",dataframe[(dataframe['CGPA'] == dataframe['CGPA'].max()) & (dataframe['PlacedOrNot']==1)]['CGPA'].values[0])  \n",
    "print(\"No of students has max CGPA and are placed: \",dataframe[(dataframe['CGPA'] == dataframe['CGPA'].max()) & (dataframe['PlacedOrNot']==1)]['CGPA'].value_counts().values[0])  \n",
    "  \n",
    "print(\"Min CGPA of Placed Person: \",dataframe[(dataframe['CGPA'] == dataframe['CGPA'].min()) & (dataframe['PlacedOrNot']==1)]['CGPA'].values[0])  \n",
    "print(\"No of students has min CGPA and are placed: \",dataframe[(dataframe['CGPA'] == dataframe['CGPA'].min()) & (dataframe['PlacedOrNot']==1)]['CGPA'].value_counts().values[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation phase\n",
    "### The process of statistical representation involves using statistical measures and visualizations to present data in a meaningful and understandable manner with the main objective of enabling the user to understand insights and patterns in the data and make well-informed decisions using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.box(dataframe, y='CGPA')  \n",
    "figure.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.box(dataframe, y='Age')  \n",
    "figure.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.box(dataframe, y=['Internships','CGPA', 'Age'])  \n",
    "figure.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical to Numerical\n",
    "### In machine learning, encoding categorical variables to numeric variables is a typical preprocessing step. It requires changing a qualitative attribute-representing category variable into a numerical variable that may be employed in mathematical operations and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Converting gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Gender'] = dataframe['Gender'].map({'Male': 1, 'Female': 0})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Converting Stream column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Stream'].unique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Stream'] = dataframe['Stream'].map({'Electronics And Communication': 1,  \n",
    "                                 'Computer Science': 2,  \n",
    "                                'Information Technology': 3,  \n",
    "                                'Mechanical':4,  \n",
    "                                'Electrical':5,  \n",
    "                                'Civil':6})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sample(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Input and Output Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.corr()['PlacedOrNot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.iloc[:,0:7]  \n",
    "y = dataframe.iloc[:,-1]  \n",
    "X  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the shape of the X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)  \n",
    "print(y.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing dataset  (70% training and 30% testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Shape of all the training and testing dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  \n",
    "print(X_test.shape)  \n",
    "print(y_train.shape)  \n",
    "print(y_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "X_train_scale = scaler.fit_transform(X_train)  \n",
    "X_test_scale = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the model\n",
    "### Training and evaluating the model are the two critical steps in machine learning that determine the accuracy and performance of the model. These steps require careful planning, attention to detail, and rigorous evaluation to develop a model that can generalize well to new, unseen data.\n",
    "\n",
    "### Here will go with different machine learning algorithms and find their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "classifier = LogisticRegression()  \n",
    "  \n",
    "# Without Scaling  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "classifier = DecisionTreeClassifier(random_state=0)  \n",
    "  \n",
    "#without scaling  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "  \n",
    "classifier = RandomForestClassifier(max_depth=10, random_state=0)  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "  \n",
    "svc = SVC()  \n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}  \n",
    "classifier = GridSearchCV(svc, parameters)  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC  \n",
    "classifier = NuSVC()  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "classifier = GaussianNB()  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "classifier = MultinomialNB()  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB  \n",
    "classifier = BernoulliNB()  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB  \n",
    "classifier = CategoricalNB()  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=3)  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier  \n",
    "classifier = SGDClassifier(max_iter=1000, tol=1e-3)  \n",
    "  \n",
    "# Without Scaling  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron  \n",
    "  \n",
    "classifier = Perceptron(tol=1e-3, random_state=0)  \n",
    "# Without Scaling  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV  \n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0)  \n",
    "  \n",
    "# Without Scaling  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without Scaling and CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"Without Scaling and With CV: \",scores.mean())  \n",
    "  \n",
    "# With Scaling  \n",
    "classifier.fit(X_train_scale,y_train)  \n",
    "y_pred = classifier.predict(X_test_scale)  \n",
    "print(\"With Scaling and Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train_scale, y_train, cv=10)  \n",
    "print(\"With Scaling and With CV: \",scores.mean())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "### After observing the scores by training different models Random Forest Classifier is considered as the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=10, random_state=0)  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"With CV: \",scores.mean())  \n",
    "print(\"Precision Score: \", precision_score(y_test, y_pred))  \n",
    "print(\"Recall Score: \", recall_score(y_test, y_pred))  \n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of the Model\n",
    "\n",
    "# Using Hyper-Parameter tuning using GridsearchCV, Hypertune the parameters for Random forest and get the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  \n",
    "    'bootstrap': [False,True],  \n",
    "    'max_depth': [5,8,10, 20],  \n",
    "    'max_features': [3, 4, 5, None],  \n",
    "    'min_samples_split': [2, 10, 12],  \n",
    "    'n_estimators': [100, 200, 300]  \n",
    "}  \n",
    "  \n",
    "rfclassifier = RandomForestClassifier()  \n",
    "  \n",
    "classifier = GridSearchCV(estimator = rfclassifier, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 1)  \n",
    "  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Accuracy: \",accuracy_score(y_test,y_pred))  \n",
    "print(classifier.best_params_)  \n",
    "print(classifier.best_estimator_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the accuracy of the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(bootstrap=False, max_depth=5,max_features=None,  \n",
    "                             min_samples_split=2,  \n",
    "                             n_estimators=100, random_state=0)  \n",
    "classifier.fit(X_train,y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "print(\"Without CV: \",accuracy_score(y_test,y_pred))  \n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)  \n",
    "print(\"With CV: \",scores.mean())  \n",
    "print(\"Precision Score: \", precision_score(y_test, y_pred))  \n",
    "print(\"Recall Score: \", recall_score(y_test, y_pred))  \n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### The likelihood that a student will be hired by a firm may be predicted using placement prediction utilizing machine learning techniques. The application of machine learning algorithms offers a more data-driven and objective approach to the hiring process, allowing businesses to find potential applicants who would have gone unnoticed using conventional hiring techniques. Machine learning is becoming more and more prevalent across a wide range of sectors, and placement prediction using machine learning algorithms is poised to become a crucial tool in the hiring process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
